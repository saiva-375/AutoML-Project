{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imvLzD-Ma8Qr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor, GradientBoostingClassifier\n",
        "from sklearn.svm import SVR, SVC\n",
        "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
        "from sklearn.metrics import mean_squared_error, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "\n",
        "# Load dataset\n",
        "def load_data(file_path):\n",
        "    data = pd.read_csv(file_path)\n",
        "    return data\n",
        "\n",
        "# Data cleaning and preprocessing\n",
        "def preprocess_data(data):\n",
        "    # Handle missing values\n",
        "    if data.isnull().sum().any():\n",
        "        print(\"Missing values found. Handling missing values...\")\n",
        "        imputer = SimpleImputer(strategy='mean')  # Replace with mean for numerical columns\n",
        "        data = pd.DataFrame(imputer.fit_transform(data), columns=data.columns)\n",
        "\n",
        "    # Remove duplicates\n",
        "    if data.duplicated().sum() > 0:\n",
        "        print(\"Duplicates found. Removing duplicates...\")\n",
        "        data = data.drop_duplicates()\n",
        "\n",
        "    # Check for outliers using IQR\n",
        "    for col in data.select_dtypes(include=[np.number]).columns:\n",
        "        Q1 = data[col].quantile(0.25)\n",
        "        Q3 = data[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "        outliers = data[(data[col] < lower_bound) | (data[col] > upper_bound)]\n",
        "        if len(outliers) > 0:\n",
        "            print(f\"Outliers found in column {col}. Handling outliers...\")\n",
        "            data[col] = np.where(data[col] > upper_bound, upper_bound, np.where(data[col] < lower_bound, lower_bound, data[col]))\n",
        "\n",
        "    # Remove skewness using log transformation\n",
        "    for col in data.select_dtypes(include=[np.number]).columns:\n",
        "        if data[col].skew() > 1 or data[col].skew() < -1:\n",
        "            print(f\"Skewness found in column {col}. Applying log transformation...\")\n",
        "            data[col] = np.log1p(data[col])\n",
        "\n",
        "    # Encode categorical data\n",
        "    categorical_cols = data.select_dtypes(include=['object']).columns\n",
        "    if len(categorical_cols) > 0:\n",
        "        print(\"Categorical columns found. Encoding categorical data...\")\n",
        "        data = pd.get_dummies(data, columns=categorical_cols, drop_first=True)\n",
        "\n",
        "    # Scale numerical data\n",
        "    scaler = StandardScaler()\n",
        "    numerical_cols = data.select_dtypes(include=[np.number]).columns\n",
        "    data[numerical_cols] = scaler.fit_transform(data[numerical_cols])\n",
        "\n",
        "    return data\n",
        "\n",
        "# Split data into training, validation, and test sets\n",
        "def split_data(data, target_col):\n",
        "    X = data.drop(target_col, axis=1)\n",
        "    y = data[target_col]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "# Train and evaluate regression models\n",
        "def evaluate_regression_models(X_train, X_val, y_train, y_val):\n",
        "    models = {\n",
        "        \"Linear Regression\": LinearRegression(),\n",
        "        \"Decision Tree\": DecisionTreeRegressor(),\n",
        "        \"Random Forest\": RandomForestRegressor(),\n",
        "        \"Gradient Boosting\": GradientBoostingRegressor(),\n",
        "        \"SVR\": SVR(),\n",
        "        \"KNN\": KNeighborsRegressor()\n",
        "    }\n",
        "    results = {}\n",
        "    for name, model in models.items():\n",
        "        print(f\"Training {name}...\")\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_val)\n",
        "        mse = mean_squared_error(y_val, y_pred)\n",
        "        results[name] = mse\n",
        "        print(f\"{name} - MSE: {mse}\")\n",
        "    return results\n",
        "\n",
        "# Train and evaluate classification models\n",
        "def evaluate_classification_models(X_train, X_val, y_train, y_val):\n",
        "    models = {\n",
        "        \"Logistic Regression\": LogisticRegression(),\n",
        "        \"Decision Tree\": DecisionTreeClassifier(),\n",
        "        \"Random Forest\": RandomForestClassifier(),\n",
        "        \"Gradient Boosting\": GradientBoostingClassifier(),\n",
        "        \"SVC\": SVC(),\n",
        "        \"KNN\": KNeighborsClassifier()\n",
        "    }\n",
        "    results = {}\n",
        "    for name, model in models.items():\n",
        "        print(f\"Training {name}...\")\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_val)\n",
        "        accuracy = accuracy_score(y_val, y_pred)\n",
        "        precision = precision_score(y_val, y_pred)\n",
        "        recall = recall_score(y_val, y_pred)\n",
        "        f1 = f1_score(y_val, y_pred)\n",
        "        results[name] = {\n",
        "            \"Accuracy\": accuracy,\n",
        "            \"Precision\": precision,\n",
        "            \"Recall\": recall,\n",
        "            \"F1-Score\": f1\n",
        "        }\n",
        "        print(f\"{name} - Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1-Score: {f1}\")\n",
        "    return results\n",
        "\n",
        "# Check for overfitting or underfitting\n",
        "def check_overfitting(model, X_train, X_test, y_train, y_test):\n",
        "    train_score = model.score(X_train, y_train)\n",
        "    test_score = model.score(X_test, y_test)\n",
        "    print(f\"Training Accuracy: {train_score}\")\n",
        "    print(f\"Testing Accuracy: {test_score}\")\n",
        "    if train_score > test_score + 0.1:\n",
        "        print(\"Warning: Model may be overfitting.\")\n",
        "    elif train_score < test_score - 0.1:\n",
        "        print(\"Warning: Model may be underfitting.\")\n",
        "    else:\n",
        "        print(\"Model is generalizing well.\")\n",
        "\n",
        "# Deploy the best model\n",
        "def deploy_model(model, file_path):\n",
        "    joblib.dump(model, file_path)\n",
        "    print(f\"Model deployed to {file_path}\")\n",
        "\n",
        "# Main function\n",
        "def main(file_path, target_col):\n",
        "    # Load data\n",
        "    data = load_data(file_path)\n",
        "\n",
        "    # Preprocess data\n",
        "    data = preprocess_data(data)\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_val, X_test, y_train, y_val, y_test = split_data(data, target_col)\n",
        "\n",
        "    # Determine if the problem is regression or classification\n",
        "    if data[target_col].nunique() > 10:  # Regression\n",
        "        print(\"Regression problem detected.\")\n",
        "        results = evaluate_regression_models(X_train, X_val, y_train, y_val)\n",
        "        best_model_name = min(results, key=results.get)\n",
        "        print(f\"Best model: {best_model_name} with MSE: {results[best_model_name]}\")\n",
        "    else:  # Classification\n",
        "        print(\"Classification problem detected.\")\n",
        "        results = evaluate_classification_models(X_train, X_val, y_train, y_val)\n",
        "        best_model_name = max(results, key=lambda x: results[x]['Accuracy'])\n",
        "        print(f\"Best model: {best_model_name} with Accuracy: {results[best_model_name]['Accuracy']}\")\n",
        "\n",
        "    # Train the best model on the full training set\n",
        "    if data[target_col].nunique() > 10:  # Regression\n",
        "        best_model = LinearRegression() if best_model_name == \"Linear Regression\" else \\\n",
        "                     DecisionTreeRegressor() if best_model_name == \"Decision Tree\" else \\\n",
        "                     RandomForestRegressor() if best_model_name == \"Random Forest\" else \\\n",
        "                     GradientBoostingRegressor() if best_model_name == \"Gradient Boosting\" else \\\n",
        "                     SVR() if best_model_name == \"SVR\" else \\\n",
        "                     KNeighborsRegressor()\n",
        "    else:  # Classification\n",
        "        best_model = LogisticRegression() if best_model_name == \"Logistic Regression\" else \\\n",
        "                     DecisionTreeClassifier() if best_model_name == \"Decision Tree\" else \\\n",
        "                     RandomForestClassifier() if best_model_name == \"Random Forest\" else \\\n",
        "                     GradientBoostingClassifier() if best_model_name == \"Gradient Boosting\" else \\\n",
        "                     SVC() if best_model_name == \"SVC\" else \\\n",
        "                     KNeighborsClassifier()\n",
        "\n",
        "    best_model.fit(X_train, y_train)\n",
        "\n",
        "    # Check for overfitting or underfitting\n",
        "    check_overfitting(best_model, X_train, X_test, y_train, y_test)\n",
        "\n",
        "    # Deploy the best model\n",
        "    deploy_model(best_model, \"best_model.pkl\")\n",
        "\n",
        "# Run the script\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = input(\"please upload the dataset:\" )  # Replace with your dataset file path\n",
        "    target_col = input(\"target_column:\" )    # Replace with your target column name\n",
        "    main(file_path, target_col)"
      ]
    }
  ]
}